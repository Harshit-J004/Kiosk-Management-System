# Multi-Store Kiosk Management System
ğŸš€ Overview
This system is an agentic AI-based kiosk manager designed to monitor, diagnose, maintain, and operate kiosks across multiple retail stores. It uses CrewAI agents, LangChain LLMs, and custom store-aware tools for intelligent decision-making and task execution.

ğŸ§  Architecture Overview
sql
Copy
Edit
[ User ]
   â”‚
   â–¼
[ Main CLI App (main) ]         â† L0: Supervisor
   â”‚
   â–¼
[ EnhancedSupervisor ]          â† L0
   â”œâ”€ OperatorApp for store001  â† L1
   â”œâ”€ OperatorApp for store002
   â”œâ”€ OperatorApp for store003
   â””â”€ OperatorApp (global)

Each OperatorApp:
   â”œâ”€ Kiosk Specialist Agent     â† L2
   â”œâ”€ Order Specialist Agent     â† L2
   â””â”€ Alert Specialist Agent     â† L2

Each Agent uses:
   â””â”€ Enhanced Tool (_run logic) â† L3 (Tool level)
âš™ï¸ Key Components
ğŸ”¹ EnhancedSupervisor (L0)
Controls all operator apps per store

Runs monitoring, emergency response, dashboards

Manages crew orchestration for each store

ğŸ”¹ EnhancedOperatorApp (L1)
Each store gets its own app instance

Builds diagnostic, operations, and maintenance crews

Each crew has 2 agents and 2 tasks run sequentially

ğŸ”¹ Agents (L2)
Multi-Store Kiosk Specialist

Multi-Store Order Specialist

Multi-Store Alert Specialist

Each agent has:

Role, goal, backstory

Custom llm (e.g., GPT)

Tools: EnhancedKioskTool, EnhancedOrderTool, EnhancedAlertTool

ğŸ”¹ Tools (L3)
Tool logic triggers directly from Task descriptions:

diagnostic, status, restart â†’ EnhancedKioskTool

process orders, metrics, priority â†’ EnhancedOrderTool

alerts, critical, resolve â†’ EnhancedAlertTool

ğŸ§© Crew + Task Flow
Each Crew is created with:

python
Copy
Edit
Crew(
  agents=[Agent1, Agent2],
  tasks=[
    Task(description="...", expected_output="...", agent=Agent1),
    Task(description="...", expected_output="...", agent=Agent2)
  ],
  process=Process.sequential
)
âœ… Agents read the task â†’ understand intent â†’ call toolâ€™s _run(query) method.

ğŸ”„ Trigger Flow (Example: Option 3 â€” Full AI Ops)
text
Copy
Edit
User selects: Run Full AI Operations (Option 3)
â†“
main() â†’ EnhancedSupervisor.run_full_operations()
â†“
For each store:
  â†’ operator_app.run_diagnostics()
    â†³ crew.kickoff() â†’ agent â†’ tool._run("diagnostic store001")

  â†’ operator_app.run_operations()
    â†³ crew.kickoff() â†’ agent â†’ tool._run("process orders store001")
ğŸ“Š Built-In Tools & Use Cases
Tool	Sample Queries	Description
EnhancedKioskTool	diagnostic store001, status	Kiosk health checks and restart
EnhancedOrderTool	process orders, revenue report	Order analytics and fulfillment
EnhancedAlertTool	critical alerts, resolve alerts	System-wide alert tracking

ğŸ› ï¸ Setup Instructions
ğŸ“¦ Dependencies
python-dotenv

langchain

langchain-openai

crewai

openai (if using GPT)

ollama (if using local models like mistral)

ğŸ” API Setup (OpenAI)
env
Copy
Edit
# .env
OPENAI_API_KEY=sk-xxxxxx
In code:

python
Copy
Edit
from dotenv import load_dotenv
load_dotenv()
ğŸ§  LLM Setup Recommendation
For best performance:

python
Copy
Edit
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",  # or "gpt-4"
    temperature=0.3
)
Avoid Ollama unless you're doing tool-only flow with expected_output disabled.

âœ… Feature Summary
ğŸ”„ Restart Systems

ğŸ§  AI Diagnostics + Operations

ğŸ› ï¸ Maintenance Scheduling

ğŸ“Š Quick Status + Revenue Analytics

ğŸš¨ Critical Alerts Management

ğŸ“ˆ Latency Tracking

ğŸ‘¥ Fully Agentic CrewAI Architecture

ğŸ§­ Sample Execution Flow
text
Copy
Edit
main()
â”œâ”€â”€ Option 3: Run Full AI Operations
â”‚   â””â”€â”€ EnhancedSupervisor.run_full_operations()
â”‚       â”œâ”€â”€ operator_app.run_diagnostics()
â”‚       â”‚   â””â”€â”€ crew.kickoff() â†’ agent â†’ tool._run("diagnostic store001")
â”‚       â””â”€â”€ operator_app.run_operations()
â”‚           â””â”€â”€ crew.kickoff() â†’ agent â†’ tool._run("process store001")
â”‚
â””â”€â”€ Option 5: Quick Status
    â””â”€â”€ operator_app.get_quick_status()
        â””â”€â”€ calls tool._run() directly (no LLM)
ğŸ“Œ Final Notes
âœ… Your code is perfectly modular and scalable

âœ… Agent reasoning works best when using OpenAI models + proper expected_output

âœ… Tools are powerful â€” they handle 100% of the store-specific logic

ğŸ§  Future scope: Add memory, database logging, Slack alerts, or vector search
